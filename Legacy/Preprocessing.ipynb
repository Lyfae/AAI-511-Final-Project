{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686f5bc2",
   "metadata": {},
   "source": [
    "___\n",
    "##### $Name:\\,\\color{blue}{Christopher\\,J.\\,Watson\\,,Nathan\\,Edwards\\,,Paul\\,Thai}$\n",
    "##### $School:\\,\\color{blue}{Marcos\\,School\\,of\\,Engineering,\\,University\\,of\\,San\\,Diego}$\n",
    "##### $Class:\\,\\color{blue}{AAI\\,511-\\,Neural\\,Networks\\,and\\,Learning}$\n",
    "##### $Assignment:\\,\\color{blue}{MSAAI\\,Final\\,Project}$\n",
    "##### $Date:\\,\\color{blue}{8/15/2023}$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc343580",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13274e2-c25d-4b74-8ebf-4c8c0c18b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "\n",
    "# File and Data Handling\n",
    "import os\n",
    "\n",
    "# Data Visualization and Numerical Computing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# MIDI Processing\n",
    "import mido\n",
    "\n",
    "# Machine Learning and Deep Learning\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Machine learning Metrics and Evaluations\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4e215",
   "metadata": {},
   "source": [
    "### Self-Defined Classes\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d67db43-0234-42f6-9509-c27f240c284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Midi Transcriber\n",
    "class MidiTranscriber():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def Sequence(self, track):\n",
    "        result = []\n",
    "        notes = {}\n",
    "        time = 0\n",
    "        final_result = []\n",
    "        data = [] \n",
    "        state = [0] * 128\n",
    "        tick_counter = 0\n",
    "        \n",
    "        tick_interval = 20\n",
    "        \n",
    "        for i in range(16):\n",
    "            data.append(state)\n",
    "            \n",
    "        for msg in track:\n",
    "            time += msg.time\n",
    "            tick_counter += msg.time\n",
    "            \n",
    "            if(tick_counter > tick_interval):\n",
    "                final_result.append(data)\n",
    "                tick_counter = 0\n",
    "            \n",
    "            if msg.type == 'note_on':\n",
    "                if msg.velocity != 0:\n",
    "                    past = notes.get(msg.note)\n",
    "                    if(past == None):\n",
    "                        past = []\n",
    "                    notes[msg.note] = past + [time , 1] \n",
    "                    result.append([time, msg.note, msg.velocity,  msg.channel, msg.type])\n",
    "                    data[msg.channel][msg.note] = msg.velocity\n",
    "                else:\n",
    "                    pass\n",
    "            if msg.type == 'note_off':\n",
    "                past = notes.get(msg.note)\n",
    "                if(past == None):\n",
    "                    past = []\n",
    "                notes[msg.note] = past + [time , 0]\n",
    "                result.append([time ,msg.note, msg.velocity ,  msg.channel, msg.type])\n",
    "                data[msg.channel][msg.note] = 0\n",
    "                \n",
    "        return result, data, final_result\n",
    "        \n",
    "        \n",
    "    def BuildArray(self,midi):\n",
    "        Lenghth = [len(tr) for tr in midi.tracks]\n",
    "        track_length = max(Lenghth)\n",
    "        min_track =  track_length/10 #Only accept tracks 1/10 of length.\n",
    "        all_arys = []\n",
    "        training = []\n",
    "        for track in range(len(midi.tracks)):\n",
    "            out = []\n",
    "            valid_track = (len(midi.tracks[track]) > min_track)\n",
    "            if(valid_track):\n",
    "                track = midi.tracks[track]\n",
    "                res, data, final_result = self.Sequence(track)\n",
    "                all_arys.append(res)\n",
    "                training.append(data)\n",
    "\n",
    "        return all_arys, data, final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec8d03a2-23e8-4487-abc9-4aca18d39fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scriber = MidiTranscriber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "727c4757-6fbb-4a01-b865-fffa4e885ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parser(composer, path):\n",
    "    midis = os.listdir(path)\n",
    "    \n",
    "    pair = []\n",
    "    \n",
    "    for file in midis:\n",
    "        if(file == \".DS_Store\"):\n",
    "            continue \n",
    "        midis_path = path + \"/\" + file\n",
    "        #print(midis_path)\n",
    "        \n",
    "        midi = mido.MidiFile(midis_path)\n",
    "        #print(len(midi.tracks[0]) + len(midi.tracks[1]) + len(midi.tracks[2]))\n",
    "        #print(len(midi.tracks[0]))\n",
    "        arr, data, final_result = scriber.BuildArray(midi)\n",
    "        \n",
    "        \n",
    "        sequence_size = 100\n",
    "        \n",
    "        div = len(final_result) / sequence_size\n",
    "        last = 0\n",
    "        for i in range(int(div)):\n",
    "\n",
    "            clip = final_result[last:sequence_size*int((i+1))]\n",
    "            last = int(sequence_size)*(i+1)\n",
    "            \n",
    "            x = clip\n",
    "            y= composer\n",
    " \n",
    "            pair.append([x,y])\n",
    "    \n",
    "            if(not len(x) == 100):\n",
    "                print(len(x))\n",
    "        \n",
    "    return arr, data, final_result, pair\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d68a388-97c5-4e14-93a3-92ff03f66f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "     \n",
    "    def enum(self,y):\n",
    "        composers = [\"bach\", \"bartok\", \"byrd\", \"chopin\", \"handel\", \"hummel\", \"mendelssohn\",\"mozart\", \"schumann\"]\n",
    "        #print(y)\n",
    "        for i in range(len(composers)):\n",
    "            if composers[i] == y:\n",
    "                slot = i\n",
    "        out = [0] * 9\n",
    "        out[slot] = 1\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def __init__(self, pair):\n",
    "        self.pair = pair\n",
    "    def __len__(self):\n",
    "        return len(self.pair)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = torch.tensor(self.pair[idx][0])\n",
    "        y = torch.tensor(self.enum(self.pair[idx][1]))\n",
    "        return x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffd187",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a13e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pairs\n",
    "def create_pairs(dataset_path, composers):\n",
    "    # Create a list to hold the data\n",
    "    pairs = []\n",
    "\n",
    "    # Iterate through the dataset, grad the data, and append to list\n",
    "    for composer in composers:\n",
    "        path = dataset_path +\"/\"+ composer\n",
    "        print(composer)\n",
    "        arr, data, final_result, pair = Parser(composer, path)\n",
    "        pairs.append(pair)\n",
    "\n",
    "    return pairs\n",
    "\n",
    "# Combine all the data into one single\n",
    "def combine_all(pairs):\n",
    "    # Create new list to hold the new data\n",
    "    new_list = []\n",
    "\n",
    "    # Iterate through the pairs and combine them\n",
    "    for i in pairs:\n",
    "        for j in i:\n",
    "            new_list.append(j)\n",
    "            \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d0145",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a38ea1d3-2dbd-4a8b-befc-2d2ec5edeeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bach\n",
      "bartok\n",
      "byrd\n",
      "chopin\n",
      "handel\n",
      "hummel\n",
      "mendelssohn\n",
      "mozart\n",
      "schumann\n",
      "bach\n",
      "bartok\n",
      "byrd\n",
      "chopin\n",
      "handel\n",
      "hummel\n",
      "mendelssohn\n",
      "mozart\n",
      "schumann\n"
     ]
    }
   ],
   "source": [
    "# Set our train and test dataset\n",
    "dataset_train = \"Composer_Dataset/NN_midi_files_extended/train\"\n",
    "dataset_test = \"Composer_Dataset/NN_midi_files_extended/test\"\n",
    "\n",
    "# List of our composers\n",
    "composers = ['bach', 'bartok', 'byrd', 'chopin', 'handel', 'hummel', 'mendelssohn', 'mozart', 'schumann']\n",
    "\n",
    "# Create the train and test pairs\n",
    "train_pairs = create_pairs(dataset_path=dataset_train,composers=composers)\n",
    "test_pairs = create_pairs(dataset_path=dataset_test, composers=composers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def01338-17be-4266-8517-a51028f4ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test pair\n",
    "train_pair = combine_all(train_pairs)\n",
    "test_pair = combine_all(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92f22077-9492-42f9-8986-42968368e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train and Test Dataset \n",
    "train_data = Dataset(train_pair)\n",
    "test_data = Dataset(test_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c456f4fd-b7e0-41ce-ac65-fb51201e4854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 16, 128])\n",
      "torch.Size([100, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "# Validate the shape of an arbitrary location to ensure it is the shape we desire\n",
    "print(train_data[1000][0].shape)\n",
    "print(test_data[100][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a66b950-70ab-4392-9e04-ce2f0ffbd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our train and test data with dataloader \n",
    "train_dataloader = DataLoader(train_data,batch_size=3,shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df820bc",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfd7922c-c399-4931-b7f2-4b598b811bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(100, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 2 * 64, 128) \n",
    "        self.fc2 = nn.Linear(128, 9)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "# Creating the Long Short Term Memory model class\n",
    "class LSTM(nn.Module):\n",
    "    # create the constructor of our RNN\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        # self initialize the hidden size LSTM, and fully connected layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Initialize the hidden state and cell state\n",
    "        hidden_state = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        cell_state = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # grabbing the ouput of the hidden state\n",
    "        output_state, _ = self.lstm(x, (hidden_state, cell_state))\n",
    "\n",
    "        # pass output state into through the fully connected layer\n",
    "        output_state = self.fc(output_state[:, -1, :])\n",
    "\n",
    "        return output_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88fe84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "model = CNN() \n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c7c99",
   "metadata": {},
   "source": [
    "### Training\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b75eec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6] Loss: 5.4209\n",
      "Epoch [2/6] Loss: 5.4199\n",
      "Epoch [3/6] Loss: 5.4193\n",
      "Epoch [4/6] Loss: 5.4189\n",
      "Epoch [5/6] Loss: 5.4187\n",
      "Epoch [6/6] Loss: 5.4185\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 6\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(torch.float)\n",
    "        labels = labels.to(torch.float)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc2f9a",
   "metadata": {},
   "source": [
    "### Evaulation and Results\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a90dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction List [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 4, 4, 4, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Labels tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model -  rough outline not done yet. \n",
    "model.eval()\n",
    "\n",
    "prediction_list = list()\n",
    "labels_list = list()\n",
    "\n",
    "# Disable gradient computation to save memory\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(torch.float)\n",
    "        labels = labels.to(torch.float)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        prediction_list.extend(predicted.numpy())\n",
    "        labels_list.extend(labels.numpy())\n",
    "\n",
    "print(\"Prediction List\", prediction_list)\n",
    "print(\"Labels\", labels)\n",
    "\n",
    "# Finding accuracy, precision, f1 score and confusion matrix\n",
    "# accuracy = accuracy_score(labels_list, prediction_list)\n",
    "# precision = precision_score(labels_list, prediction_list, average='weighted')\n",
    "# f1_score = f1_score(labels_list, prediction_list, average='weighted')\n",
    "# confusion_mtrx = confusion_matrix(labels_list, prediction_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
